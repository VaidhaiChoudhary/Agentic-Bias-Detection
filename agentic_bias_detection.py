# -*- coding: utf-8 -*-
"""Agentic Bias Detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fP5IqpO9bHqQE69fLiVpVag4ScWhqOym

### **College Admissions Agent with Bias Critic**

This notebook demonstrates an AI admissions **advisor agent** that provides balanced, nuanced answers to the question:

"Who should get preference in college admissions?"

It also includes a **Bias Critic** that reviews the agent's response to detect any potential bias, especially regional or social biases.

**How the Agent Works**

The Agent considers multiple perspectives about college admissions:

* Legacy and donor preferences

* Underrepresented groups and affirmative action

* Merit-based admissions

* Holistic review combining academic achievement, diversity, and personal traits

It provides arguments and counterarguments to offer a balanced and fair viewpoint.

**Role of the Bias Critic**

- The Bias Critic evaluates the agent’s response to check for any bias or unfair emphasis on certain groups or regions.

- It ensures that the agent's statement is neutral, fair, and inclusive.

- It also provides suggestions to improve the statement’s clarity and fairness.
"""

!pip install --upgrade openai

"""This code demonstrates how to use two AI agents working together for college admissions:

1. **Admissions Advisor Agent** — generates a thoughtful, balanced response to a question about who should get preference in college admissions.

2. **Bias Critic Agent** — reviews the advisor’s response to detect any regional or social bias and provides feedback to ensure fairness.

We use the Groq-powered OpenAI API client to run these agents with the `llama3-70b-8192` model.
"""

from openai import OpenAI

# OpenAI client - Groq
client = OpenAI(
    api_key="YOUR API KEY",
    base_url="https://api.groq.com/openai/v1"
)

# Agent 1: Generates a response
def agentic_generator(prompt):
    response = client.chat.completions.create(
        model="llama3-70b-8192",
        messages=[
            {"role": "system", "content": "You are an admissions advisor."},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content

# Agent 2: Bias detection agent
def agentic_bias_critic(agent_response):
    critique_prompt = (
        f"Your goal is to identify any regional bias in the following statement:\n\n"
        f"'{agent_response}'\n\n"
        "Explain whether the statement contains bias and why."
    )
    response = client.chat.completions.create(
        model="llama3-70b-8192",
        messages=[
            {"role": "system", "content": "You are a fairness and bias detection expert."},
            {"role": "user", "content": critique_prompt}
        ]
    )
    return response.choices[0].message.content

#
prompt = "Who should get preference in college admissions?"
agent_response = agentic_generator(prompt)
bias_feedback = agentic_bias_critic(agent_response)

print("User Prompt:", prompt)
print("\nAgent Response:", agent_response)
print("\nBias Critic's Verdict:\n", bias_feedback)

"""**Understanding the Output**

- **User Prompt:** The question we asked the admissions advisor.

- **Agent Response:** The detailed, balanced answer the advisor generated. It includes multiple perspectives and counterarguments about admissions preferences.

- **Bias Critic's Verdict:** The fairness expert's review of the advisor’s response. It highlights strengths and suggests minor improvements to avoid potential bias.

Together, these agents simulate a fair and transparent decision-making discussion, demonstrating how AI can support balanced reasoning with bias awareness.

#### **Iterative AI Admissions Advisor with Bias Feedback Loop**

This code implements a two-agent system that interacts iteratively:

1. **Admissions Advisor Agent** generates a response to the question about college admissions preference.

2. **Bias Critic Agent** reviews the advisor’s response for any regional or other bias, providing feedback.

The advisor then revises its answer considering the feedback, improving fairness and balance.

This loop runs up to two iterations or stops early if no bias is detected.

We use the Groq-powered OpenAI client with the `llama3-70b-8192` model.
"""

from openai import OpenAI

client = OpenAI(
    api_key="YOUR API KEY",
    base_url="https://api.groq.com/openai/v1"
)

def generate_answer(prompt, context=""):
    messages = [
        {"role": "system", "content": "You are an admissions advisor providing a balanced response."},
    ]
    if context:
        messages.append({"role": "system", "content": f"Please revise your answer considering the feedback: {context}"})
    messages.append({"role": "user", "content": prompt})

    response = client.chat.completions.create(
        model="llama3-70b-8192",
        messages=messages
    )
    return response.choices[0].message.content

def critique_answer(answer):
    prompt = (
        f"Review the following admissions advisor's statement for regional or other bias:\n\n"
        f"\"{answer}\"\n\n"
        "If you find bias, explain it and suggest how to improve it. If no bias, say so."
    )

    response = client.chat.completions.create(
        model="llama3-70b-8192",
        messages=[
            {"role": "system", "content": "You are a fairness and bias detection expert."},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content

# Agentic loop
prompt = "Who should get preference in college admissions?"
answer = ""
feedback = ""

for i in range(2):
    answer = generate_answer(prompt, context=feedback)
    print(f"\nIteration {i+1} - Agent Response:\n{answer}")
    feedback = critique_answer(answer)
    print(f"\nIteration {i+1} - Bias Critic Feedback:\n{feedback}")

    if "no bias" in feedback.lower():
        print("\nNo bias detected. Stopping iterations.")
        break

"""**Understanding the Output**

- Each iteration shows the **Agent Response** — the advisor’s current answer to the admissions question.

- The **Bias Critic Feedback** follows, pointing out any biases or confirming fairness.

- If the critic finds bias, the next iteration asks the advisor to revise the response based on the feedback.

- The loop stops early if no bias is detected, ensuring the final answer is fair and balanced.

This interactive approach demonstrates how AI agents can collaborate to improve answer quality and fairness through feedback.

"""

